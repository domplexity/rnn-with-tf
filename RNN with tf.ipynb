{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-10-17 21:48:37--  https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
      "Resolving ocw.mit.edu... 104.86.58.148, 2a02:26f0:e9:287::18a8, 2a02:26f0:e9:290::18a8\n",
      "Connecting to ocw.mit.edu|104.86.58.148|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5458199 (5.2M) [text/plain]\n",
      "Saving to: 't8.shakespeare.txt.1'\n",
      "\n",
      "t8.shakespeare.txt. 100%[===================>]   5.21M  3.62MB/s    in 1.4s    \n",
      "\n",
      "2016-10-17 21:48:40 (3.62 MB/s) - 't8.shakespeare.txt.1' saved [5458199/5458199]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('t8.shakespeare.txt') as file:\n",
    "    raw_lines = [line.replace('\\r',' ') for line in file.readlines()]\n",
    "raw_text = ' '.join(raw_lines[100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_size = len(raw_text)\n",
    "vocab_size = len(set(raw_text))\n",
    "num_neurons = 200\n",
    "num_steps = 100\n",
    "batch_size = 2000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to generate batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_batch(raw_data, batch_size, num_steps):\n",
    "\n",
    "    if batch_size % num_steps != 0:\n",
    "        warnings.warn(\"Warning: batch size is not a multiple of num_steps.\")\n",
    "    \n",
    "    raw_y = raw_data[1:] + raw_data[0]\n",
    "    \n",
    "    # partition raw data into batches and stack them vertically in a data matrix\n",
    "    num_batch = data_length // batch_size\n",
    "    data_x = np.zeros([num_batch, batch_size], dtype=np.int32)\n",
    "    data_y = np.zeros([batch_size, batch_partition_length], dtype=np.int32)\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        data_x[i] = raw_data[batch_size * i:batch_size * (i + 1)]\n",
    "        data_y[i] = raw_y[batch_size * i:batch_size * (i + 1)]\n",
    "    \n",
    "    # further divide batch partitions into num_steps for truncated backprop\n",
    "    batch_lines = batch_size // num_steps\n",
    "\n",
    "    for i in range(num_batch):\n",
    "        for n in range(batch_lines):\n",
    "            x = data_x[i, n * num_steps:(n + 1) * num_steps]\n",
    "            y = data_y[i, n * num_steps:(n + 1) * num_steps]\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [batch_size // num_steps, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size // num_steps, num_steps], name='labels_placeholder')\n",
    "init_state = tf.zeros([batch_size // num_steps, num_neurons])\n",
    "\n",
    "x_one_hot = tf.one_hot(x, vocab_size)\n",
    "#rnn_inputs = tf.unpack(x_one_hot, axis=1)\n",
    "\n",
    "cell = tf.nn.rnn_cell.BasicRNNCell(num_neurons)\n",
    "rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=init_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_length = int(y.get_shape()[1])\n",
    "out_size = int(y.get_shape()[2])\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([num_neurons, out_size], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "\n",
    "output = tf.reshape(output, [-1, num_neurons])\n",
    "prediction = tf.nn.softmax(tf.matmul(output, weight) + bias)\n",
    "prediction = tf.reshape(prediction, [-1, max_length, out_size])\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(\n",
    "    target * tf.log(prediction), reduction_indices=[1])\n",
    "cross_entropy = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(self):\n",
    "        learning_rate = 0.003\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate)\n",
    "        return optimizer.minimize(self.cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_epochs, num_steps, state_size=4, verbose=True):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps)):\n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            if verbose:\n",
    "                print(\"\\nEPOCH\", idx)\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                tr_losses, training_loss_, training_state, _ = \\\n",
    "                    sess.run([losses,\n",
    "                              total_loss,\n",
    "                              final_state,\n",
    "                              train_step],\n",
    "                                  feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "                if step % 100 == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step\", step,\n",
    "                              \"for last 250 steps:\", training_loss/100)\n",
    "                    training_losses.append(training_loss/100)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    batch = gen_batch(raw_data, batch_size, num_steps)\n",
    "    sess.run(model.optimize, {data: batch, target: y, dropout: 0.5})\n",
    "    error = sess.run(model.error, {data: test.data, target: test.target, dropout: 1})\n",
    "    print('error {:3.1f}%'.format(100 * error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.variable_scope('softmax'):\n",
    "    W = tf.get_variable('W', [state_size, num_classes])\n",
    "    b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "logits = [tf.matmul(rnn_outputs, W) + b ]\n",
    "predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "y_as_list = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(1, num_steps, y)]\n",
    "\n",
    "loss_weights = [tf.ones([batch_size]) for i in range(num_steps)]\n",
    "losses = tf.nn.seq2seq.sequence_loss_by_example(logits, y_as_list, loss_weights)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (for tensorflow)",
   "language": "python",
   "name": "python-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
